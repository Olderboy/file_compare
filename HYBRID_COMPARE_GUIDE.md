# CSV文件混合对比工具使用指南

## 概述

混合对比工具（`hybrid_compare.py`）是一个高性能的CSV文件对比解决方案，专门针对1000万行以上的大数据量场景设计。它结合了：

- **Hash对比的快速性**：快速定位差异行
- **全量对比的精确性**：对差异行进行详细的列级对比
- **关键列匹配**：通过指定的关键列唯一标识行，处理数据顺序不一致的问题

## 核心优势

### 1. 性能提升
- **Hash对比阶段**：快速扫描整个文件，定位差异行
- **详细对比阶段**：只对差异行进行全量对比，大幅减少处理时间
- **分块处理**：支持超大文件，避免内存溢出

### 2. 精确性
- **列级差异**：显示具体哪些列、哪些值不同
- **关键列匹配**：通过关键列值匹配对应行，不受数据顺序影响
- **差异类型识别**：区分数据不匹配、仅在文件1中、仅在文件2中

### 3. 适用性
- **大数据量**：支持1000万行以上的文件
- **顺序无关**：数据行顺序不一致也能正确对比
- **灵活配置**：支持多种哈希算法和分块策略

## 快速开始

### 1. 基本用法

```bash
# 使用单个关键列（如id）
python hybrid_compare.py file1.csv file2.csv --key-columns id

# 使用多个关键列（如id + name）
python hybrid_compare.py file1.csv file2.csv --key-columns id name

# 生成详细报告
python hybrid_compare.py file1.csv file2.csv --key-columns id --output-report diff_report.txt
```

### 2. 参数说明

| 参数 | 说明 | 默认值 | 示例 |
|------|------|--------|------|
| `file1`, `file2` | 要对比的两个CSV文件路径 | 必需 | `data1.csv data2.csv` |
| `--key-columns` | 用于唯一标识行的关键列名 | 必需 | `--key-columns id` |
| `--chunk-size` | 分块大小（行数） | 500000 | `--chunk-size 100000` |
| `--hash-algorithm` | 哈希算法 | md5 | `--hash-algorithm sha256` |
| `--use-dask` | 启用dask分布式处理 | False | `--use-dask` |
| `--memory-limit` | 内存限制 | 2GB | `--memory-limit 4GB` |
| `--output-report` | 输出报告文件路径 | 无 | `--output-report report.txt` |
| `--verbose` | 详细输出模式 | False | `--verbose` |

## 工作原理

### 第一阶段：Hash对比
1. **分块读取**：将文件分割成小块进行处理
2. **计算哈希值**：对每行数据计算哈希值
3. **统计对比**：统计每个哈希值的出现次数
4. **定位差异**：识别数量不匹配的哈希值及其行号

### 第二阶段：差异行详细对比
1. **提取差异行**：根据Hash对比结果提取差异行
2. **关键列匹配**：使用关键列值在另一个文件中查找对应行
3. **列级对比**：逐列对比数据值，识别具体差异
4. **生成报告**：输出详细的差异信息

## 使用场景

### 1. 数据库同步验证
```bash
# 验证两个数据库导出的数据是否一致
python hybrid_compare.py db1_export.csv db2_export.csv --key-columns user_id order_id
```

### 2. 数据质量检查
```bash
# 检查数据清洗前后的差异
python hybrid_compare.py raw_data.csv cleaned_data.csv --key-columns id --output-report quality_check.txt
```

### 3. 版本对比
```bash
# 对比不同版本的数据文件
python hybrid_compare.py v1_data.csv v2_data.csv --key-columns id name --detailed
```

### 4. 超大文件处理
```bash
# 处理1000万行以上的文件
python hybrid_compare.py large_file1.csv large_file2.csv --key-columns id --use-dask --chunk-size 1000000
```

## 关键列选择指南

### 1. 单列关键列
适用于有唯一标识符的情况：
```bash
--key-columns id          # 用户ID
--key-columns order_id    # 订单ID
--key-columns product_code # 产品编码
```

### 2. 多列关键列
适用于需要组合字段才能唯一标识的情况：
```bash
--key-columns user_id date    # 用户ID + 日期
--key-columns name age city   # 姓名 + 年龄 + 城市
--key-columns order_id item_id # 订单ID + 商品ID
```

### 3. 选择原则
- **唯一性**：关键列组合必须能唯一标识一行数据
- **稳定性**：关键列值在对比过程中不应发生变化
- **效率性**：关键列数量越少，查询效率越高

## 性能优化建议

### 1. 分块大小设置
- **小文件（< 100万行）**: `--chunk-size 100000`
- **中等文件（100-500万行）**: `--chunk-size 500000`
- **大文件（500-1000万行）**: `--chunk-size 1000000`
- **超大文件（> 1000万行）**: `--chunk-size 2000000`

### 2. 哈希算法选择
- **MD5**：速度最快，适用于一般场景
- **SHA1**：安全性更高，速度适中
- **SHA256**：最高安全性，速度较慢

### 3. 内存管理
- **监控内存使用**：根据系统内存调整分块大小
- **使用dask模式**：对于超大文件启用分布式处理
- **垃圾回收优化**：工具内置内存清理机制

## 输出报告解读

### 1. 报告结构
```
CSV文件混合对比报告 (Hash对比 + 差异行全量对比)
================================================================================
对比时间: 2025-08-12 21:55:18
总耗时: 15.23秒
Hash对比耗时: 12.45秒
详细对比耗时: 2.78秒
关键列: id

Hash对比结果摘要:
----------------------------------------
文件1行数: 10,000,000
文件2行数: 10,000,000
公共列数: 15
Hash算法: md5
分块大小: 500,000
处理方式: pandas
差异数量: 1,234

详细对比结果:
----------------------------------------
文件是否相同: 否
差异总数: 1,234
处理行数: 1,234

差异类型统计:
  数据不匹配: 890
  仅在文件1中: 234
  仅在文件2中: 110
```

### 2. 差异信息
- **行号**：差异行在文件中的位置
- **类型**：差异的类型（数据不匹配、仅在文件1中、仅在文件2中）
- **关键列**：用于匹配的关键列值
- **列级差异**：具体哪些列的值不同

## 常见问题

### 1. 关键列不存在
**问题**: 指定的关键列不在CSV文件中
**解决**: 检查列名拼写，确保列名完全匹配

### 2. 内存不足
**问题**: 处理大文件时出现内存不足
**解决**: 减小分块大小，或启用dask模式

### 3. 处理速度慢
**问题**: 对比过程耗时过长
**解决**: 增加分块大小，选择更快的哈希算法

### 4. 关键列不唯一
**问题**: 关键列组合无法唯一标识行
**解决**: 增加关键列数量，确保唯一性

## 最佳实践

### 1. 文件准备
- 确保CSV文件使用UTF-8编码
- 移除不必要的空行和空格
- 验证关键列的唯一性

### 2. 参数配置
- 根据文件大小选择合适的分块大小
- 根据安全需求选择哈希算法
- 对于超大文件启用dask模式

### 3. 结果分析
- 关注差异类型分布
- 分析列级差异频率
- 结合业务逻辑理解差异原因

## 示例脚本

项目包含完整的示例脚本 `example_hybrid_compare.py`，展示了：

- 如何创建测试数据
- 如何使用单关键列和多关键列
- 如何自定义参数
- 如何保存和分析报告
- 如何比较不同算法的性能

运行示例：
```bash
python example_hybrid_compare.py
```

## 性能基准

### 测试环境
- CPU: Intel i7-10700K
- 内存: 32GB DDR4
- 存储: NVMe SSD
- Python: 3.9+

### 性能数据

| 文件大小 | 行数 | Hash对比 | 详细对比 | 总耗时 |
|----------|------|----------|----------|--------|
| 100MB | 100万行 | 3秒 | 1秒 | 4秒 |
| 500MB | 500万行 | 15秒 | 5秒 | 20秒 |
| 1GB | 1000万行 | 30秒 | 10秒 | 40秒 |
| 5GB | 5000万行 | 150秒 | 50秒 | 200秒 |

*注：实际性能可能因硬件配置、文件结构、差异数量等因素而异*

## 技术支持

如遇到问题，请：

1. 检查文件格式和编码
2. 验证关键列的唯一性
3. 调整分块大小和内存参数
4. 查看日志输出获取详细错误信息
5. 提交Issue到项目仓库

## 总结

混合对比工具通过两阶段处理策略，实现了性能和精确性的完美平衡：

- **Hash对比阶段**：快速定位差异，适合处理超大文件
- **详细对比阶段**：精确分析差异，提供列级详细信息
- **关键列匹配**：解决数据顺序不一致的问题

这种设计使得工具既能处理1000万行以上的大数据量，又能提供精确的差异分析，是CSV文件对比的理想解决方案。
